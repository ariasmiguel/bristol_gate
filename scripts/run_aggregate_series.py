#!/usr/bin/env python3
"""
Aggregate Series Creation Entry Point
Standalone script to create aggregate series using the integrated DuckDB pipeline
Saves output in Parquet format with timestamps to data/silver/ following medallion architecture
"""

import argparse
import logging
import sys
from pathlib import Path
from datetime import datetime

from src_pipeline.pipelines.aggregate_series import create_aggregate_series_from_interpolated_data
from src_pipeline.core.date_utils import DateUtils

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def main():
    """Main function for aggregate series creation using DuckDB integration"""
    parser = argparse.ArgumentParser(
        description='Create Aggregate Series using DuckDB Integration Pipeline',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    python run_aggregate_series.py                                       # Full pipeline with defaults
    python run_aggregate_series.py --start-date 2020-01-01               # Filter from specific date  
    python run_aggregate_series.py --output data/silver/final_data.parquet  # Save to custom file
    python run_aggregate_series.py --method staged                       # Use pandas pivot method

What this does:
    â€¢ Loads data directly from DuckDB staging tables
    â€¢ Applies interpolation to create daily time series  
    â€¢ Creates aggregate/computed series (ratios, differences, normalized values)
    â€¢ Inserts new symbol metadata into DuckDB symbols table
    â€¢ Saves enhanced dataset in Parquet format to data/silver/

File Output:
    â€¢ Creates timestamped file: final_aggregated_data_YYYYMMDD_HHMMSS.parquet
    â€¢ Creates/updates latest file: final_aggregated_data.parquet
    
Key improvements:
    â€¢ No intermediate CSV files needed
    â€¢ Direct DuckDB integration for better performance  
    â€¢ Automatic symbol metadata management
    â€¢ Parquet format for efficient storage and faster loading
    â€¢ Timestamped files for audit trail
    â€¢ Follows medallion architecture (bronze â†’ silver â†’ gold)
        """
    )
    
    parser.add_argument('--db-path', default='bristol_gate.duckdb',
                       help='Path to DuckDB database file')
    parser.add_argument('--start-date', default='1950-01-01',
                       help='Start date for filtering (YYYY-MM-DD)')
    parser.add_argument('--usrec-symbol', default='USREC',
                       help='Recession indicator symbol for special handling')
    parser.add_argument('--method', choices=['direct', 'staged'], default='direct',
                       help='Interpolation method: direct (SQL pivot) or staged (pandas pivot)')
    parser.add_argument('--output', default='data/silver/final_aggregated_data.parquet',
                       help='Output Parquet file path (will add timestamp)')
    parser.add_argument('--skip-save', action='store_true',
                       help='Skip saving Parquet output (useful for testing)')
    
    args = parser.parse_args()
    
    # Validate database file exists
    db_path = Path(args.db_path)
    if not db_path.exists():
        logger.error(f"âŒ Database file not found: {db_path}")
        logger.error("ğŸ’¡ Run 'python setup_duckdb.py' first to initialize the database")
        sys.exit(1)
    
    # Ensure output directory exists
    if not args.skip_save:
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        logger.info(f"ğŸ“ Silver layer directory ready: {output_path.parent}")
    
    logger.info("ğŸ§® DuckDB-Integrated Aggregate Series Creator")
    logger.info(f"â° Started at: {DateUtils.format_current_datetime()}")
    logger.info(f"ğŸ—ƒï¸ Database: {db_path}")
    logger.info(f"ğŸ“… Filter start: {args.start_date}")
    logger.info(f"ğŸ”§ Method: {args.method}")
    logger.info(f"ğŸ“‰ USREC symbol: {args.usrec_symbol}")
    
    if not args.skip_save:
        logger.info(f"ğŸ’¾ Output (Parquet with timestamp): {args.output}")
    else:
        logger.info("âš ï¸ Parquet output will be skipped")
    
    try:
        # Run integrated pipeline
        result = create_aggregate_series_from_interpolated_data(
            db_path=str(db_path),
            filter_start_date=args.start_date,
            usrec_symbol=args.usrec_symbol,
            interpolation_method=args.method,
            output_path=args.output if not args.skip_save else None
        )
        
        if result.empty:
            logger.error("âŒ Pipeline returned empty data")
            sys.exit(1)
        
        logger.info("ğŸ‰ Pipeline completed successfully!")
        logger.info(f"ğŸ“Š Final dataset shape: {result.shape}")
        logger.info(f"ğŸ“… Date range: {result.index.min()} to {result.index.max()}")
        logger.info(f"ğŸ“ˆ Total columns: {len(result.columns)}")
        
        # Show sample of aggregate series created
        calc_columns = [col for col in result.columns if any(
            agg_name in col for agg_name in [
                'RSALESAGG', 'BUSLOANS_by_GDP', 'DGS10_to_DGS2', 
                'GSPC_Close_by_MDY_Close', 'GDP_by_POPTHM'
            ]
        )]
        
        if calc_columns:
            logger.info(f"ğŸ§® Sample aggregate series: {calc_columns[:5]}")
        
        if not args.skip_save:
            logger.info(f"ğŸ’¾ Enhanced data saved with timestamp and latest versions")
        
        logger.info("âœ… Aggregate series and symbols metadata updated in DuckDB!")
        logger.info("ğŸ¥ˆ Data saved to Silver layer (medallion architecture)")
        
        # Show efficiency gains vs CSV
        logger.info("ğŸ’¡ Benefits of timestamped Parquet format:")
        logger.info("   â€¢ ~3-5x smaller file size vs CSV")
        logger.info("   â€¢ ~10-50x faster loading for analytics")
        logger.info("   â€¢ Preserves data types and metadata")
        logger.info("   â€¢ Column-oriented storage for better compression")
        logger.info("   â€¢ Full audit trail with timestamps")
        logger.info("   â€¢ Always-available 'latest' version for downstream processes")
        
    except KeyboardInterrupt:
        logger.info("â¹ï¸ Process interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ Process failed: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main() 